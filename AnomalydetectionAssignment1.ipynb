{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZpZv19g949s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is anomaly detection and what is its purpose?\n",
        "\n",
        "Ans = Anomaly detection, also known as outlier detection, is a technique in data analysis and machine learning aimed at identifying rare items, events, or observations that significantly differ from the majority of the data. The purpose of anomaly detection is to pinpoint unusual patterns or anomalies in a dataset, which may indicate errors, fraud, novel phenomena, or other issues. Anomalies are data points that deviate from the norm, and their detection can have various applications like :\n",
        "\n",
        "1. **Fraud Detection**: Identifying fraudulent transactions, such as credit card fraud or insider trading, by detecting unusual patterns in financial data.\n",
        "\n",
        "2. **Network Security** : Detecting unusual network traffic patterns that may indicate cyberattacks, intrusions, or malware infections.\n",
        "\n",
        "3. **Industrial Equipment Monitoring** : Identifying anomalies in sensor data from machines and equipment to detect potential failures or maintenance needs.\n",
        "\n",
        "4. **Healthcare** : Detecting abnormal patient vital signs or medical test results to identify potential diseases or health issues.\n",
        "\n",
        "5. **Quality Control** : Ensuring the quality of products in manufacturing by identifying defective items or processes.\n",
        "\n",
        "6. **Environmental Monitoring** : Detecting unusual environmental measurements, such as pollution levels or weather patterns, for early warning systems.\n",
        "\n",
        "7. **Image and Video Analysis** : Identifying anomalous objects or activities in images or video feeds for surveillance and security applications.\n",
        "\n",
        "8. **IoT Devices** : Monitoring data from Internet of Things (IoT) devices to detect anomalies in home automation, smart cities, and more."
      ],
      "metadata": {
        "id": "dXfA4rz8_0QA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkehacEZ_9Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q2. What are the key challenges in anomaly detection?\n",
        " Ans = Anomaly detection, the process of identifying oultliers or unusual patterns in data, comes with several key challenges:\n",
        "\n",
        "1. **Imbalanced Data**: Anomalies are typically rare events compared to normal data. This class imbalance can lead to models that are biased towards the majority class, making it challenging to detect anomalies effectively.\n",
        "2. **Feature Selection**: Choosing relevant features that capture the characteristics of both normal and anomalous data is crucial. Poor feature selection can lead to suboptimal anomaly detection performance.\n",
        "3. `Noise and Variability`: Real-world data often contains noise and natural variability, which can make it difficult to distinguish between anomalies and normal variations. Anomaly detection models need to be robust to such variations.\n",
        "4. `Model Selection`: Selecting the most appropriate anomaly detection algorithm for a given dataset and problem can be challenging. Different techniques may perform better or worse depending on the data characteristics.\n",
        "5. `Scalability`: Anomaly detection often needs to be performed on large datasets, potentially in real-time or near real-time. This requires scalable algorithms and efficient computation.\n",
        "6. `Concept Drift`: Data distributions may change over time due to various factors. Anomaly detection models must be capable of adapting to these changes to avoid false alarms or missed anomalies.\n",
        "7. `Interpretability`: Understanding and interpreting the reasons behind detected anomalies can be complex, especially in high-dimensional data. Interpretable anomaly detection is crucial for taking appropriate actions.\n",
        "8. `Evaluation Metrics`: Selecting appropriate evaluation metrics for anomaly detection can be challenging. Common metrics like precision, recall, F1-score, and ROC curves may not always be suitable, especially when dealing with imbalanced datasets.\n",
        "9. `Scarcity of Anomalies`: Anomalies may be extremely rare, making it challenging to collect enough labeled examples for model training and evaluation."
      ],
      "metadata": {
        "id": "r5gESA-B_9hd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-LAJITvADex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
        "\n",
        " Ans = Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches to identifying anomalies in data, and they differ in several key ways:\n",
        "\n",
        "1. **Labeling of Data:**\n",
        "\n",
        "   - **Unsupervised Anomaly Detection:** In unsupervised anomaly detection, the algorithm works with unlabeled data, meaning that it doesn't have access to any information about which data points are normal or anomalous. The goal is to identify patterns or data points that deviate significantly from the majority of the data.\n",
        "\n",
        "   - **Supervised Anomaly Detection:** In supervised anomaly detection, the algorithm is trained on a labeled dataset where each data point is explicitly labeled as normal or anomalous. The algorithm learns to distinguish between the two classes based on the labeled examples.\n",
        "\n",
        "2. **Training Process:**\n",
        "\n",
        "   - **Unsupervised Anomaly Detection:** Unsupervised methods don't require training with labeled data. They typically rely on statistical, clustering, or density-based techniques to identify anomalies based on the distribution of data points.\n",
        "\n",
        "   - **Supervised Anomaly Detection:** Supervised methods require a training phase where the algorithm learns from labeled data. This training phase involves building a model (e.g., a classifier) that can predict whether a data point is normal or anomalous based on features extracted from the data.\n",
        "\n",
        "3. **Availability of Anomalous Data:**\n",
        "\n",
        "   - **Unsupervised Anomaly Detection:** Unsupervised methods can detect anomalies even when anomalous data is scarce or unavailable during training. They identify deviations from what is considered normal within the dataset itself.\n",
        "\n",
        "   - **Supervised Anomaly Detection:** Supervised methods rely on the availability of labeled anomalous data for training. If the training data lacks representative examples of anomalies, the model's performance may be limited.\n",
        "\n",
        "4. **Applicability:**\n",
        "\n",
        "   - **Unsupervised Anomaly Detection:** Unsupervised methods are often used when there is limited prior knowledge of what constitutes an anomaly in the data. They are useful for exploring and identifying unknown patterns or outliers.\n",
        "\n",
        "   - **Supervised Anomaly Detection:** Supervised methods are used when the types of anomalies are well-defined and labeled examples of anomalies are available. They are suitable for cases where specific types of anomalies need to be detected with high precision.\n",
        "\n",
        "5. **Performance Evaluation:**\n",
        "\n",
        "   - **Unsupervised Anomaly Detection:** Evaluating the performance of unsupervised methods can be challenging since there are no ground-truth labels for anomalies. Evaluation often involves measures like silhouette scores, density estimation, or expert validation.\n",
        "\n",
        "   - **Supervised Anomaly Detection:** The performance of supervised methods can be assessed using standard classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC, as they operate as classifiers.\n",
        "\n",
        "6. **Scalability:**\n",
        "\n",
        "   - **Unsupervised Anomaly Detection:** Unsupervised methods, such as clustering-based approaches, can be more scalable to large datasets since they don't require the manual labeling of data.\n",
        "\n",
        "   - **Supervised Anomaly Detection:** Supervised methods require labeled training data, which can be time-consuming and expensive to collect for large datasets.\n"
      ],
      "metadata": {
        "id": "dY6ccKTmAFr2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zD9AR__lAJ7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are the main categories of anomaly detection algorithms?\n",
        "\n",
        "Ans= Anomaly detection algorithms can be broadly categorized into the following main categories:\n",
        "\n",
        "1. **Statistical Methods:**\n",
        "   - **Z-Score (Standard Score):** Measures how many standard deviations a data point is from the mean. Data points with z-scores exceeding a threshold are considered anomalies.\n",
        "   - **Modified Z-Score:** A variation of the standard z-score that is robust to outliers.\n",
        "   - **Percentiles/Quantiles:** Anomalies are detected by comparing data points to predefined percentiles or quantiles of the data distribution.\n",
        "   - **Grubbs' Test:** Detects univariate outliers by comparing a data point to the sample mean and standard deviation.\n",
        "\n",
        "2. **Distance-Based Methods:**\n",
        "   - **Euclidean Distance:** Measures the distance between data points in Euclidean space. Data points far from others are potential anomalies.\n",
        "   - **Mahalanobis Distance:** Accounts for correlations between variables when measuring distance. Useful for multivariate data.\n",
        "   - **Cosine Similarity:** Measures the cosine of the angle between data points. Used for high-dimensional data, text, and document analysis.\n",
        "   - **K-Nearest Neighbors (KNN):** Considers the distance to the K nearest neighbors of a data point. Data points with distant neighbors may be anomalies.\n",
        "\n",
        "3. **Density-Based Methods:**\n",
        "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** Identifies clusters based on dense regions of data. Outliers are data points not in any cluster.\n",
        "   - **OPTICS (Ordering Points To Identify the Clustering Structure):** An extension of DBSCAN that provides a hierarchical view of clusters.\n",
        "   - **LOF (Local Outlier Factor):** Measures the density of data points compared to their neighbors. Low-density points are potential outliers.\n",
        "   - **Mean-Shift:** Identifies modes in the data's probability density function, with data points far from modes considered anomalies.\n",
        "\n",
        "4. **Clustering-Based Methods:**\n",
        "   - **K-Means Clustering:** After clustering data, data points not belonging to any cluster or in small clusters may be anomalies.\n",
        "   - **Hierarchical Clustering:** Similar to K-means, data points not in any cluster or in small clusters can be anomalies.\n",
        "   - **Support Vector Machine:** Trains a model on normal data and classifies data points as normal or anomalies. Useful when only normal data is available.\n",
        "\n",
        "5. **Machine Learning-Based Methods:**\n",
        "   - **Isolation Forest:** Constructs a decision tree by randomly selecting features and splitting points. Anomalies are isolated quickly in shallow trees.\n",
        "   - **Random Forest:** A modified version of the random forest algorithm can be used for anomaly detection by considering the out-of-bag (OOB) error.\n",
        "   - **Autoencoders:** Neural networks are trained to reconstruct input data. Anomalies result in larger reconstruction errors.\n",
        "   - **Deep Generative Models:** Variational autoencoders (VAEs) and generative adversarial networks (GANs) can learn the data distribution and detect anomalies based on deviations.\n",
        "\n",
        "6. **Time Series-Specific Methods:**\n",
        "   - **Seasonal Decomposition of Time Series (STL):** Decomposes time series into seasonal, trend, and residual components, and anomalies are detected in the residuals.\n",
        "   - **ARIMA (AutoRegressive Integrated Moving Average):** Models time series data and identifies anomalies by comparing predicted values to actual values.\n",
        "   \n",
        "7. **Deep Learning-Based Methods:**\n",
        "   - **Deep Autoencoders:** Deep neural networks with multiple encoding and decoding layers are used to capture complex patterns in the data.\n",
        "   - **Recurrent Neural Networks (RNNs):** Particularly useful for sequential data, RNNs can capture temporal dependencies and identify anomalous sequences."
      ],
      "metadata": {
        "id": "RAT6Hat9AKUd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4TQh1NHAVup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
        "\n",
        " Ans = Assumptions made while using Distance-based anomaly detection:\n",
        "\n",
        "1. **Distance Metric:** They rely on a distance metric (e.g., Euclidean distance) to measure similarity or dissimilarity between data points.\n",
        "\n",
        "2. **Spherical Clusters:** They assume that clusters are roughly spherical or have similar densities in all directions.\n",
        "\n",
        "3. **Constant Density:** Some assume constant density within clusters, which may not hold for varying-density clusters.\n",
        "\n",
        "4. **Symmetry:** They assume symmetric distances, which means the distance from A to B is the same as from B to A.\n",
        "\n",
        "5. **Independence:** They assume independence of attributes, which may not hold for correlated features.\n",
        "\n",
        "6. **Homogeneous Data:** They assume all normal data points belong to the same distribution.\n",
        "\n",
        "7. **Single Scale:** They treat all attributes equally, which can be problematic with varying scales.\n",
        "\n",
        "8. **Noisy Data:** They may struggle to distinguish anomalies from noisy data.\n",
        "\n",
        "9. **Known Clusters:** Some require specifying the number of clusters in advance."
      ],
      "metadata": {
        "id": "3UxeDB7ZAXuW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_fRU-g6AbkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q6. How does the LOF algorithm compute anomaly scores?\n",
        "\n",
        " Ans = The LOF (Local Outlier Factor) algorithm computes anomaly scores by comparing the local density of data points with the density of their neighbors.\n",
        "\n",
        "LOF algorithm steps include :\n",
        "\n",
        "\n",
        "**Step 1 : Local Density Estimation:** For each data point in the dataset, LOF first estimates its local density. This is typically done using a distance metric, such as Euclidean distance, to measure the proximity of a point to its neighbors. The local density of a point is inversely proportional to the average distance to its k nearest neighbors, where k is a user-defined parameter.\n",
        "\n",
        "**Step 2 : Local Reachability Density:** LOF then computes the local reachability density for each data point. This is a measure of how a point's local density compares to the local densities of its neighbors. It is calculated as the ratio of a point's local density to the average local density of its k nearest neighbors.\n",
        "\n",
        "**Step 3 : LOF Score Calculation** Finally, the LOF score for each data point is computed as the average local reachability density of its k nearest neighbors. A data point with an LOF score significantly higher than 1 is considered an anomaly, as it has a lower local density compared to its neighbors.\n",
        "\n",
        "LOF identifies anomalies based on the idea that anomalies are data points with a significantly different local density compared to their neighbors. Points with LOF scores greater than 1 are considered outliers. LOF is particularly useful for detecting anomalies in datasets with varying cluster densities and complex structures."
      ],
      "metadata": {
        "id": "TmiM4l1gAdQl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HP6yvWQXAg4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
        "\n",
        "Ans= The Isolation Forest algorithm is an anomaly detection method that works by isolating anomalies (outliers) from the majority of the data. It achieves this by building an ensemble of decision trees. Here's an explanation of the key parameters in the Isolation Forest algorithm:\n",
        "\n",
        "1. **Contamination (contamination):**\n",
        "   - **Purpose:** The contamination parameter is used to specify the expected proportion of anomalies (outliers) in the dataset. It helps the algorithm set a threshold for classifying data points as anomalies or normal observations.\n",
        "   - **Options:**\n",
        "     - auto (default): The algorithm estimates the contamination based on the assumption that anomalies are rare in the dataset. It calculates the contamination as 0.1 / n_samples, where n_samples is the total number of data points.\n",
        "     - float: You can manually specify the desired contamination level as a float between 0 and 0.5. For example, setting contamination=0.05 indicates that you expect 5% of the data to be anomalies.\n",
        "\n",
        "2. **Number of Estimators (n_estimators):**\n",
        "   - **Purpose:** This parameter determines the number of decision trees in the ensemble.\n",
        "   - **Default:** The default value is 100, but you can adjust it based on the size and complexity of your dataset.\n",
        "   - **Impact:** Increasing the number of estimators can lead to a more robust and accurate model but may also increase computation time.\n",
        "\n",
        "3. **Max Samples (max_samples):**\n",
        "   - **Purpose:** The max_samples parameter controls the number of samples drawn from the dataset to build each decision tree.\n",
        "   - **Default:** The default value is auto, which means it is set to min(256, n_samples) by default.\n",
        "   - **Impact:** A smaller max_samples value can lead to more isolation and potentially better anomaly detection but may also increase variability in the results. A larger value can result in more stable but potentially less accurate models."
      ],
      "metadata": {
        "id": "9wt047c4AhSc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OC5BtqMgAmrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
        "\n",
        " Ans = In K-Nearest Neighbors (KNN) for anomaly detection, the anomaly score of a data point is typically determined by measuring the distance between that data point and its k-nearest neighbors. Anomalies are often identified as data points with neighbors that are significantly farther away from them in comparison to the majority of data points.\n",
        "\n",
        "In this case, the data point has only 2 neighbors of the same class within a radius of 0.5. Since K=10, we need to find the distance between the data point and its 10th nearest neighbor. If the data point has only 2 neighbors within a radius of 0.5, it is unlikely that it will have 10 neighbors within the same radius. Therefore, we cannot compute the anomaly score of the data point using KNN with K=10.\n",
        "\n",
        "However, if we still want to compute the anomaly score using KNN with K=10, we can extend the distance radius until we find 10 neighbors.\n",
        "\n",
        "For example, if we extend the radius to 1, we may find 10 neighbors. We can then compute the distance between the data point and its 10th nearest neighbor and use it to compute the anomaly score. The larger the distance, the higher the anomaly score.\n",
        "\n",
        "Anomaly Score = 1 / (average distance to k nearest neighbors)"
      ],
      "metadata": {
        "id": "1zqYSIVaAnJd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRvFensgAr46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n",
        "\n",
        "Ans = The Isolation Forest algorithm generates a forest of decision trees, where each data point is isolated in a different partition of the feature space. The anomaly score of a data point is computed based on the average path length of the data point in the trees of the forest.\n",
        "\n",
        "If a data point has an average path length of 5.0 compared to the average path length of the trees, we can compute its anomaly score using the following formula:\n",
        "\n",
        "Anomaly Score = 2^(-average path length / c(n))\n",
        "where c(n) is a constant that depends on the number of data points n in the dataset. The value of c(n) can be computed as:\n",
        "\n",
        "c(n) = 2 * H(n-1) - (2 * (n-1) / n)\n",
        "- where H(n-1) is the harmonic number of n-1.\n",
        "\n",
        "For a dataset of 3000 data points, c(n) can be computed as:\n",
        "\n",
        "c(3000) = 2 * H(2999) - (2 * 2999 / 3000) = 11.8979\n",
        "\n",
        "Using this value of c(n), we can compute the anomaly score of the data point with an average path length of 5.0 as:\n",
        "\n",
        "Anomaly Score = 2^(-5.0 / 11.8979) = 0.5017\n",
        "\n",
        "This indicates that the data point is less anomalous than a data point with an average path length that is farther from the average path length of the trees."
      ],
      "metadata": {
        "id": "K4eXjMRmAtrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing anomaly score for the datapoint which has an average path length of 5\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "\n",
        "# Generate a dataset of 3000 data points with 10 features\n",
        "X = np.random.randn(3000, 1)\n",
        "\n",
        "# Fit an Isolation Forest model with 100 trees\n",
        "isol = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
        "isol.fit(X)\n",
        "\n",
        "avg_path_length = 5.0\n",
        "anomaly_score = isol.score_samples([[avg_path_length]])[0]\n",
        "\n",
        "print(f\"The anomaly score of the data point is {anomaly_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHVMMWt_Axin",
        "outputId": "b3e8985d-5308-4aed-b519-30b91e098444"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The anomaly score of the data point is -0.7702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rD9gPdOrA0tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "To7FZXrGA13b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing anomaly score on each data point and then computing mean\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "\n",
        "# Generate a dataset of 3000 data points with 10 features\n",
        "X = np.random.randn(3000, 10)\n",
        "\n",
        "# Fit an Isolation Forest model with 100 trees\n",
        "clf = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
        "clf.fit(X)\n",
        "\n",
        "# Compute the anomaly scores for the data points\n",
        "anomaly_scores = clf.score_samples(X)\n",
        "\n",
        "# Print the anomaly scores\n",
        "print(anomaly_scores)\n",
        "\n",
        "\n",
        "# Compute the mean of the anomaly scores\n",
        "mean_anomaly_score = np.mean(anomaly_scores)\n",
        "\n",
        "# Print the mean anomaly score\n",
        "print(f\"\\nThe mean anomaly score is {mean_anomaly_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZs6qtbeA4iN",
        "outputId": "2ba678d5-a34e-4ecb-dd55-c5c1e4ece569"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.52194295 -0.45513019 -0.40764285 ... -0.48368343 -0.4140298\n",
            " -0.4824528 ]\n",
            "\n",
            "The mean anomaly score is -0.4364\n"
          ]
        }
      ]
    }
  ]
}